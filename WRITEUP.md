# Damn Vulnerable Web App DevOps Lab

## Disclaimer
Some tools in this lab have the potential to be used maliciously. The OWASP ZAP vulnerability scanner was run exclusively on a web application locally hosted on my machine. I did not scan any 3rd party systems, and I do not condone any unethical use of OWASP ZAP or any other pentesting tool. While I may discuss different types of attacks and their methodology, I am only supplying information readily available on cybersecurity sites like OWASP. I do not intend to encourage any type of illegal or unethical cyberattack, this project was purely for personal learning. 

## Use of AI  
I wanted to take a moment to explain my approach to using AI on this project. I did manually perform each part of this project, however, I did take input from AI at multiple points. Tools and project scope were based on AI suggestions, along with some of the goals of this project. Additionally, I relied on AI for troubleshooting, especially with cAdvisor, blackbox, Grafana and GitHub Actions. In particular, some of the YAML seen in GitHub Actions was generated by AI or based off of results from AI. ChatGPT was the main AI used in this project, but I also relied on the Google AI search results at certain points, especially regarding GitHub Actions. Additionally, I tried to document instance of AI use in this project, but I am not sure I exhaustively noted them all. I tried to paraphrase and not directly copy what AI told me into this report, but some lines may contain words directly copied from AI. I believe AI can be a good learning tool, as long as it is not overrelied on. 

## Introduction
I selected this project to build hands-on experience with real-world DevOps and security technologies including containerization, vulnerability scanning, monitoring and CI/CD. My goal was primarily to learn, not necessarily to set up an environment that would be identical to something seen in enterprise IT. I chose to deploy a web application designed to be vulnerable inside of a docker container. I wanted to pentest it in order to find security insights, and from there I decided to create Prometheus alerts to help monitor the system. Afterwards, I made a Grafana dashboard, though I was limited to monitoring internal Prometheus metrics. Lastly I deployed a web app service and added vulnerability scanning to learn about CI/CD principles.  
For this lab, I used the following tools:
- Damn Vulnerable Web App (DVWA) - A web app which has intentional security vulnerabilities. It features different levels of security to learn pentesting. 
- OWASP ZAP - An open source vulnerability scanner and pentesting tool. I chose it for its beginner friendly nature and well-documented functionality. 
- Docker/Docker Desktop - Used to deploy and manage containers.
- Prometheus/Grafana - For monitoring containerized applications.
- GitHub Actions - To simulate a CI/CD pipeline 
- Wireshark - Used to capture packets at certain points during pentesting.
- Linux Mint - My host operating system of choice for this project.
  
I also created the following learning goals before attempting this project:
1. Build hands-on familiarity with containerized applications
2. Conduct a vulnerability scan and penetration testing to build understanding of how security vulnerabilities are discovered.
3. Create a “patch” for vulnerabilities, by changing the security level of DVWA and redeploying through CI/CD pipeline
4. ChatGPT Suggestion - automate security scanning via OWASP ZAP of the web application during the CI/CD pipeline
5. Implement monitoring using Grafana and/or Prometheus in order to alert on key metrics

## Pentesting
Before beginning pentesting, I began by pulling a containerized version of DVWA. I assigned it a random port, and configured it to use the lowest security version. I started pentesting by running an automated scan. Initially, I found 2 medium risk alerts including “Content Security Policy (CSP) Not Set (12)” and “Missing Anti-clickjacking Header (10)”. Additionally, I found 5 low risk alerts and 4 informational alerts. I reran the automated scan as I forgot to take a screenshot and found the following results: 
<img width="596" height="317" alt="Screenshot of OWASP ZAP automated scan with 2 medium risk, 4 low risk and 3 informational alerts." src="https://github.com/user-attachments/assets/ff8e710f-dc6b-47a8-9ba0-e4b980487f8c" />  


Afterwards, I moved on to a manual scan in which I logged in and clicked through each page to find the following vulnerabilities: 
<img width="550" height="385" alt="Screenshot of OWASP ZAP automated scan with 7 medium risk, 8 low risk and 5 informational alerts." src="https://github.com/user-attachments/assets/63b581c8-15dc-4234-8e7f-ed85bfd458d1" />

Moving on, I began actively trying to exploit vulnerabilities. While the vulnerability scans did give quite a bit of information, I chose to use the documentation from DVWA instead to begin pentesting. DVWA is divided into different sections for each type of cyberattack, and there are undocumented attack types as well.  
<img width="727" height="558" alt="Screenshot showing Damn Vulnerable Web Application UI including attack types on the left like Brute Force, SQL Injection, XSS (Reflected) among others, along with a description of the app on the right" src="https://github.com/user-attachments/assets/e1fc8552-d854-4df0-8526-8b7d4b84dfbf" />  

I chose to start with some of the more simple vulnerabilities. I began with a command injection, which simply took an input with an additional command afterwards. This allowed me to list files inside of the directory, which I otherwise could not have done: 
<img width="702" height="138" alt="Screenshot showing successful command injection because several files were listed in addition to ping results" src="https://github.com/user-attachments/assets/433d2843-791e-4c07-8624-cc7e2cea9401" />  

Afterwards, I created a simple SQL injection that would return all records, allowing me to see:  
<img width="206" height="43" alt="Screenshot showing SQL injection results of a first name and last name being returned." src="https://github.com/user-attachments/assets/90eb2718-67a9-4289-b14b-f4b449450f64" />  

While I am not creating fixes for either of these vulnerabilities, it is relatively straightforward to do so. Taking user input can be risky, thus sanitizing that input before use is important. For example, we can use parameterization with SQL queries to prevent SQL from interpreting malicious user input as code. I am not sure of the exact method of doing this for command injections. There are also tools like Amazon Web Services Web Access Firewall (AWS WAF) which can detect and prevent attacks like SQL injections in a cloud environment.   
From there, I wanted to perform a brute force/password spraying attack. There were several methods I considered when planning this exercise: 
- Write a Python script using Selenium to manually log in with an exhaustive amount of possible passwords. This might work, but Python and Selenium bring a lot of overhead (specifically with the browser) so I decided to use a different option
- Write a Python script using Scapy to directly send a spoofed packet to the server, less overhead than Selenium but will take additional time to write and test. I did capture a packet with Wireshark to use as a reference, but chose a different method.
- Simulate a password spoofing attack by manually trying a few known passwords in the text entry box. This would work, but I felt I could learn more by adopting a different method.
In the end, I decided to use the OWASP ZAP fuzz tool. I wanted to build more hands-on pentesting experience, and I felt that exposure to additional tools would help me build confidence.  I began by running a manual scan, logging in, and attempting 1 failed login to the brute force vulnerability. From there, I pulled a list of the 20 most common passwords from wearedevelopers.com and intentionally moved “password” to the very end as I knew it was the correct password. This simulation is somewhere between a password spraying attack and a brute foce attack. Password spraying attacks often use only a couple of common passwords, while brute force attacks tend to use dramatically more unique passwords. I chose 20 to see how the fuzz tool worked. Specifically for the fuzz tool, I had the input replace the password in the URL (wrong is being replaced here):  
<img width="935" height="23" alt="Screenshot showing the fuzz request, including a username of admin and a password of wrong" src="https://github.com/user-attachments/assets/6b6d7cb8-1127-4e45-92b3-7a0290e655f0" />  

Here are the results of running the fuzzing:  
<img width="950" height="223" alt="Screenshot showing the results of the fuzzing, the last word fuzzed was password, which has a symbol showing it was reflected" src="https://github.com/user-attachments/assets/da8bbd74-e08b-4b2c-a1e8-24f10894f0e8" />  

We can see that the final input “password” was reflected. According to Google AI, reflected means that the input data was found in the output, which could suggest a possible vulnerability. This does not guarantee a vulnerability, and I would need to do more research to find out more. I also captured the packet using Wireshark, which showed the password in plaintext because the URL contained it. We can clearly see password=password: 
<img width="940" height="46" alt="Screenshot showing a snippet of Wireshark where the username of admin and password of password are easily readable" src="https://github.com/user-attachments/assets/a1f84027-2d33-4fe8-828c-96a9a0f43ea9" />  

To prevent password spraying attacks, enforcing a strong password policy is necessary. Password complexity should be strong enough in order to prevent common passwords from being used. To prevent brute forcing attacks, monitoring systems should be in place that alert on sudden spikes in login attempts, which can increase visibility and potentially automatically block suspicious IP addresses. Lastly, passwords should be salted and hashed to increase their security. Sending plaintext passwords over potentially insecure networks is an inherent security concern. By salting and hashing the password before transmission, the original password is not recoverable. Salting (adding random data before creating a digital hash of the password) helps make it much more difficult to brute force the password by hashing values and comparing them to the known hash.  
From there, I performed a Stored Cross-Site Scripting (XSS) attack by using an embedded script. This type of attack can be prevented by using tools like AWS WAF which can inspect for malicious input. I do not know the specifics of preventing XSS attacks in the code itself. I then moved on to a File Upload exercise which I initially struggled with, as I did not understand the goal of the section. Ultimately, I performed the most basic version of a malicious file upload by simply uploading a PHP script. The application requests an image file, but doesn’t actually verify the file is an image. Once again, it is critical to the security of an application to ensure input is not malicious. Lastly, I performed a directory traversal attack. While this attack is not specifically documented in DVWA, it exists as an extension of other attacks. I simply went back to the command injection attack tab, and changed my input to include a path to a different directory. By doing so, I was able to print a list of files from a different folder than the one in the first:  
<img width="714" height="115" alt="Screenshot showing the results of a ping request plus a list of different files from the first command injection" src="https://github.com/user-attachments/assets/f6484cfe-54fd-49bc-b568-790d6b4856f3" />  

Afterwards, I moved on to monitoring with Prometheus. 

## Prometheus Monitoring and Setup
When creating my Prometheus container, I chose to use a randomly assigned port, which would later present issues. I began by going through a portion of the “Getting Started” guide provided by the Prometheus developers. I ran a couple of example queries, then began planning how to monitor DVWA. I did some research (in part with Google AI) and found that Prometheus scrapes a /metrics endpoint which DVWA does not have. I asked ChatGPT for some potential ideas on what to monitor, and got some suggestions:
- ChatGPT advised me to use node_explorer/cAdvisor to monitor for system resource usage
- It also suggested monitoring DB statistics using mysqldb_explorer, which it also told me could tie to SQL injection fuzzing or other attacks
- Additionally, it suggested running a web proxy to monitor HTTP codes
- Lastly, it proposed using blackbox to probe endpoints, which could monitor response time, status code, and uptime.
After some research (in part with Google AI), I decided to use cAdvisor for container monitoring. This would prove to be a monumental ordeal.
### cAdvisor Troubleshooting
I began by looking for cAdvisor on Docker Hub, but the only version I found was deprecated. Additionally, I found a version on Google Cloud, but this would require a Google Cloud account, which I did not wish to create. I decided to use the install command listed on the GitHub account for cAdvisor, but I was initially confused as the container did not show up in Docker Desktop. For the time being, I decided to use cAdvisor as it was, though I later created a different container. With some extensive help from ChatGPT, I fixed a network issue which stopped Prometheus from getting metrics from cAdvisor. I changed the prometheus.yml file to include the IP address assigned via DHCP. While this did allow Prometheus to scrape metrics from cAdvisor, they were limited to metrics only pertaining to the host system. With more help from ChatGPT, I learned that cAdvisor was not running in the same docker context as my other containers. I went through and deleted the cAdvisor instances I made before from the “default” context, to remove any ports or name conflicts. I tried changing my context to desktop-linux and running it again, but this didn’t seem to make a difference. Next, I put the creation command in a script, and tried adding --context desktop-linux (ChatGPT suggestion) but I got an error. ChatGPT then explained to me that I should not run this command as sudo, because sudo may not see the desktop-linux context. This allowed cAdvisor to run in the same docker context, but presented a new issue where cAdvisor only saw information about itself. I continued troubleshooting for a while, with ChatGPT’s help. According to it, this seems to be an issue with how newer versions of cAdvisor get information from running containers. From there, I did try to use the older version of cAdvisor on Docker Hub, but it was so old that it caused an error, likely due to an incompatibility between that older container and Docker Desktop (ChatGPT, which also explained how to download images that were grayed out).  
Overall I relied heavily on ChatGPT for this troubleshooting, but I feel I learned quite a bit about containers. Understanding Docker contexts allowed me to troubleshoot this issue, and it was satisfying to fix my script by not relying on sudo, which could not see the docker-desktop context. Ultimately, I did not get cAdvisor working, and thus focused on other metrics.

### Prometheus Blackbox Setup
Prometheus Blackbox is a tool that attempts to make an HTTP connection in order to get HTTP and connection related metrics. When creating the container, I chose to give blackbox a dedicated port of 55100 which helped keep things consistent. Initially, I ran into the same problem where I could not connect to the web server using localhost on the blackbox container. Instead, I had to use host.docker.internal:55000 (ChatGPT suggestion) which allowed this container to access the web server running on port 55000. This did create one issue: the port of the web server is not dedicated, meaning if another service started before the web server, that service would use port 55000. While this would be an easy issue to fix, it does mimic a real-world scenario. In the real-world, many services have dependencies, which means deployment order may matter.  
Initially, I tried making my own HTTP endpoint, but decided to use one of the default ones in blackbox. I ran into an issue with my prometheus.yml configuration, but ChatGPT explained that the relabel_configs section was required, and from there, I was able to get it working. This is an excerpt of my prometheus.yml file, containing my configs and comments showing my learning journey: 
<img width="922" height="736" alt="Prometheus YAML file with targets for prometheus, cadvisor, and blackbox" src="https://github.com/user-attachments/assets/f055589c-e6ee-4754-a03e-8acd394c0747" />



## Prometheus Alerts
When creating my Prometheus alerts, I had some help from Google AI with the syntax of the Prometheus config file to include the rule file. I may have also had help from AI with the syntax of the rule files themselfs. I initially created two alerts: the first checks if the blackbox probe has not been successful in one minute, and the second checks if the probe_duration_seconds metric of the blackbox probe is over 0.1 seconds in all probes for one minute. If the blackbox probe was unsuccessful for one minute, the system probably is not working correctly and thus I made this a critical alert. If the blackbox probe was being resolved slowly, the web server may be overloaded, which I indicated by making this alert warning level.  
I wanted to test my alerts to ensure they were working correctly. Testing the first alert was simple, I just had to stop the web server container, which resulted in the alert firing:  
<img width="891" height="511" alt="Screenshot showing the network-probe-failed-alert firing, it is a severity level critical alert" src="https://github.com/user-attachments/assets/c449627d-a391-4d58-babd-95baf7879a41" />  

The other alert was slightly more tricky to test. To do so, I went back to the OWASP ZAP fuzz tool, and used a large amount of input. I began by generating a batch of random words, then pasting them multiple times for a total of 2100 words. I began fuzzing and found the probe took around 0.0022 seconds on average before fuzzing and around 0.01 during fuzzing. These numbers were based on a small sample size so they may not be totally accurate. I tried increasing the fuzzing size, first to 30,001 words and then to 100,001 words. I used a list of 10,000 English words from MIT to do this. Even after all of this fuzzing, I could not get the alert to fire:  
<img width="928" height="311" alt="Screenshot showing the network-probe-time-alert as inactive, it is a severity level warning alert" src="https://github.com/user-attachments/assets/20d26db5-8b5e-4d45-be8f-57c926e9f4ae" />  

This may suggest the timing threshold on this alert needs to be tweaked. To test further, I would try to run a stress test on the host system, to see if that would increase the probe duration and fire this alert. Lastly, I decided to create one final alert which looked at the HTTP status code of the alert. It is possible a web server may be running, but consistently throwing errors. We would want to be aware of this behavior, and my “network-probe-failed-alert” would not help in this situation. This alert is also critical level, because it suggests a possible service interruption. Once again, I tested this alert by simply stopping the DVWA container. Since the container was not running, the HTTP status code was 0, which caused the alert to fire:  
<img width="960" height="544" alt="Screenshot showing the http-status-code-error alert firing, it is a severity level critical alert" src="https://github.com/user-attachments/assets/ae2cb237-fb99-4f72-92ad-ca9f7952955f" />  

Lastly, here is a look at the rule definition YAML file:  

<img width="832" height="714" alt="Screenshot showing rule definitions for network-probe-time-alert, network-probe-failed-alert and http-status-code-error" src="https://github.com/user-attachments/assets/976df4c7-be8b-4ef5-bc46-f60aea49aba6" />  


## Grafana 
When configuring Grafana to read from Prometheus, I first tried to use host.docker.internal:55001. I then tried localhost:55001, but that didn’t work, likely because the Grafana container would be looking at its own port of 55001. Instead, I simply relied on the DHCP-given IP address of this machine, with port 9090 which initially seemed to work. I was able to get a connection to Prometheus, but I did not see any blackbox metrics. I tried working with ChatGPT to troubleshoot the issue, but nothing worked out. It suggested using \_\_name__ =~ probe to troubleshoot possible label issues, but this returned nothing.  
I still wanted to build a dashboard to become more familiar with Grafana, so I focused on the Prometheus metrics I had. I used the CPU seconds over the last 10 minutes, HTTP duration in seconds and the query queue time, also in seconds. For the query queue time, I only focused on the data that had a slice of queue_time. I wanted to add a section on the Prometheus version (which you can see as the green “1” at the bottom, referring to 1 total data point) but I could not parse the specific version. If you mouse over the “1” it gives information including the version. I wanted just the version number reported, rather than the "1". Interestingly, I think the version reported by Grafana might be different from the version listed in the Prometheus instance I am running. I don't see how this could be true, because I am only running one Prometheus instance, so this may be a mistake on my part. I honestly don’t know why my Prometheus instance is not working with Grafana, but this is the final dashboard I was able to create:  

<img width="933" height="551" alt="Screenshot showing Grafana dashboard with an About Section explaining two of the metrics used, plus graphs for Rate of CPU Seconds used by Prometheus Over 10 miin, Prometheus HTTP Duration (Seconds), Prometheus Engine Query Queue Time Duration (Seconds) and a Prometheus version info section at the bottom which says 1" src="https://github.com/user-attachments/assets/c89e2c21-b158-42a0-b98d-991d385e6fd5" />  


## CI/CD Simulation With GitHub Actions
To start, I asked ChatGPT for some ideas on ways to incorporate GitHub Actions to mimic a CI/CD pipeline. It gave me a series of suggestions including pulling and testing a container, validating it and automating security scanning. From there, it gave me some help setting up my GitHub repository as I forgot to add a file before committing. I began by focusing on pulling the image from Docker Hub. I had some help from Google AI, as I could not find documentation on the specific syntax. I initially ran this GitHub Actions workflow using a cron job that ran every 10 minutes. I pulled the Docker image using docker pull. (I had Google AI and possibly ChatGPT help with this code)  
 <img width="665" height="429" alt="Image showing YAML to pull the latest DVWA image, with the command docker pull vulnerable/web-dvwa. It runs as a cronjob every 10 minutes, and is built on ubuntu-latest" src="https://github.com/user-attachments/assets/85f1f42f-bd42-4761-9358-ce50bb4dcbdb" />

I had some help from ChatGPT ensuring that the container was being pulled successfully using docker ps -a. Google AI helped me run docker after actually pulling it, but I ran into issues.

I consulted ChatGPT which explained the concept of services to me. It suggested I use one for DVWA and gave me the code to implement one. Services are useful for testing workflow operations. There was an issue where ChatGPT gave me a curl statement that initially failed, but later gave revised code and insight for either repeating the curl statement or simply waiting a few seconds. With a little syntax help from Google AI, I changed to using workflow_dispatch to run my workflow with the press of a button rather than every 10 minutes. I wanted to add more to the pipeline, as I felt I would benefit from more hands-on experience. I decided I wanted to add automated vulnerability scanning to this project. with ChatGPT suggested tools like trivy, which I decided to use. At the start of this project, I considered scanning Docker images automatically with OWASP ZAP, but using trivy is very straightforward and realistic in this workflow. Initially, I struggled with getting my container to run properly, as I installed trivy, but could not get it to run correctly:  
<img width="513" height="139" alt="Screenshot showing a run section of a GitHub Actions workflow with docker run aquasec/trivy newline trivy fs ." src="https://github.com/user-attachments/assets/c75ef679-be66-4ca3-8355-5e8f406066b5" />
  
ChatGPT explained this is not running in the container itself, which is why it is encountering some issues. With some help from Google AI, I formatted my workflow to use the “container” keyword which allowed the container to run the scan:

```
name: Pull Docker Image

on:
  workflow_dispatch
  # schedule:
  #   # Run every 10 minutes, just for testing
  #   - cron: "*/10 * * * *"

#ChatGPT gave me the code for build-dvwa
jobs:
  build-dvwa:
    needs: vulnerability-scanner
    runs-on: ubuntu-latest
    services:
      web-app:
        image: vulnerables/web-dvwa
        ports:
          - 8080:80
    steps:
      - name: Check if running correctly
       # | denotes that this is a multiple line string
        run: | #ping http://localhost:8080
         docker ps -a
         sleep 5
         curl -v http://localhost:8080
         
  vulnerability-scanner:
    #needs: build
    runs-on: ubuntu-latest
    container:
      image: aquasec/trivy
    steps:
      # - with:
      #     image: aquasec/trivy
       - run: | 
           trivy image vulnerables/web-dvwa
```
And a look at some of the results:  
<img width="932" height="170" alt="Screenshot showing results of the vulnerability scan of dvwa, in which 1575 total security alerts were found, including 12 unknown,, 116 low, 642 medium, 551 high and 254 critical" src="https://github.com/user-attachments/assets/647d04aa-2987-46a0-a467-60aa95ffbe06" />  

These results aren’t very surprising as this web application is specifically designed to have security vulnerabilities. Lastly, I made the “build-dvwa” job require the “vulnerability-scanner” job so that it only pulls the image after it has actually been scanned.  
While I did rely on AI to help learn syntax and GitHub Actions basics, it was really satisfying to see the pipeline actually work: trivy automatically scanned the Docker image before it was pulled. This is similar to a real-world DevSecOps scenario in which both security and CI/CD are relevant concerns. 

Reference:
This is a resource that I found helpful for this section, so I wanted to document it:  
https://aschmelyun.com/blog/using-docker-run-inside-of-github-actions/
I also used the GitHub forums, documentation and other resources for this section. Other unlisted resources may have been used for other parts of this lab.
## Lessons Learned
Looking at the goals I started this project with, I feel I have met many of them. I definitely gained hands-on experience with Docker containers. Although I feel like I have more to learn, I am more confident with the technology than when I started. I was able to conduct a vulnerability scan of DVWA, and I successfully exploited several basic vulnerabilities. I was also able to add a vulnerability scan to the CI/CD pipeline, though it was not with OWASP ZAP. I successfully implemented alerting with Prometheus to monitor key metrics. I feel that I understand the technology better and I am ready to work with it in other scenarios. While I could not access blackbox metrics in Grafana, I was still able to create a dashboard to increase container visibility. I made a CI/CD pipeline which automatically pulled the DVWA from Docker Hub. I didn’t do the “deploy a patch” part of this goal, but that was in part because I did not understand how changing the security level of DVWA worked. The DVWA security level is set in the web page itself, though it may be possible to change another way. Automatically pulling the latest version of a container is similar to deploying a security patch in the real-world, so I feel that process accomplished the same goal.  
While this project wasn’t exactly the same as a real-world enterprise DevSecOps environment, I feel the tools and skills I learned will position me well to succeed in enterprise IT. I’m more familiar with containers, monitoring and alerting, and cybersecurity and I'm excited to learn about their real-world applications. 

